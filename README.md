# Optimization Techniques for Machine Learning
This project explores various optimization techniques used in machine learning, including iterative algorithms such as gradient descent, Newton's methods, and advanced strategies like backtracking line search.
All this notebooks were done to optimization curse of the CS master program.

## Description

This project focuses on the implementation and analysis of optimization techniques commonly applied in machine learning tasks. Key features include:

- Implementation of gradient-based methods for optimizing machine learning models.

- Application of Newton's methods for faster convergence using second-order derivatives.

- Utilization of backtracking line search to dynamically adjust step sizes in gradient-based methods.

- Comparative analysis of convergence rates and computational efficiency of the algorithms.

These approaches are tested on functions with smooth and non-smooth properties to evaluate their robustness and efficiency.

## **Prerequisites**  
- Python >= 3.8.  
- Required libraries:  
  - `numpy`  
  - `matplotlib`

---

## **Installation**  
Steps to set up and run this project:  
1. Clone this repository:  
   ```bash
   git clone https://github.com/angelmdezhdez/optimization-curse.git  
